version: '3.8'

# ============================================================================
# PRODUCTION ML OBSERVABILITY STACK
# ============================================================================
#
# Services included:
# 1. Fraud Detection API (our ML service)
# 2. Redis (feature store cache)
# 3. OpenTelemetry Collector (telemetry aggregation)
# 4. Jaeger (distributed tracing UI)
# 5. Prometheus (metrics storage & alerting)
# 6. Grafana (dashboards & visualization)
# 7. Loki (log aggregation)
# 8. Alertmanager (alert routing & deduplication)
# 9. Kafka + Zookeeper (event streaming - optional)
#
# ARCHITECTURE:
#
#   [Fraud API] ──metrics──> [OTel Collector] ──> [Prometheus] ──> [Grafana]
#        │                          │                                  ▲
#        └──traces──────────────────┴──────────────> [Jaeger] ────────┘
#        └──logs───────────────────────────────────> [Loki] ──────────┘
#
# CRITICAL: Resource Limits
# --------------------------
# We set memory limits to prevent any one service from consuming all RAM:
# - Prometheus: 1GB (can store ~1M time series)
# - Grafana: 512MB (UI rendering)
# - Loki: 512MB (log storage)
#
# In production, you'd size based on actual usage:
# - 1M predictions/day = ~10K metrics = 100MB Prometheus
# - 10M logs/day = ~500MB Loki
#
# COST ANALYSIS:
# This entire stack runs on:
# - Development: 1 machine (4GB RAM, 2 CPU) = $40/month
# - Production: 3 machines (8GB RAM, 4 CPU each) = $360/month
#
# vs. DataDog: $15/host/month × 3 = $45/month base + $0.10 per custom metric
#              At 10K custom metrics = $45 + $1000 = $1045/month
#
# ============================================================================

services:
  # ==========================================================================
  # 1. FRAUD DETECTION API (Our ML Service)
  # ==========================================================================
  fraud-api:
    build:
      context: .
      dockerfile: docker/Dockerfile.fraud-api
    container_name: fraud-api
    ports:
      - "8000:8000"  # API endpoint
    environment:
      # Service config
      - ENVIRONMENT=development
      - MODEL_PATH=/app/models/fraud_model.pkl

      # Dependencies
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - OTLP_ENDPOINT=http://otel-collector:4317

      # Observability
      - LOG_LEVEL=INFO
      - MODEL_NAME=fraud_detector_v1
      - MODEL_VERSION=1.0.0

    depends_on:
      - redis
      - otel-collector

    volumes:
      - ./src:/app/src
      - ./models:/app/models

    # Health check (Kubernetes-style)
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s

    # Resource limits (prevent runaway memory usage)
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'

    networks:
      - ml-observability

    restart: unless-stopped

  # ==========================================================================
  # 2. REDIS (Feature Store Cache)
  # ==========================================================================
  redis:
    image: redis:7-alpine
    container_name: redis
    ports:
      - "6379:6379"

    # Redis persistence (so cache survives restarts)
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru

    volumes:
      - redis-data:/data

    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 3

    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'

    networks:
      - ml-observability

    restart: unless-stopped

  # ==========================================================================
  # 3. OPENTELEMETRY COLLECTOR (Telemetry Aggregation Hub)
  # ==========================================================================
  # WHY: Decouples your app from backend services
  # Instead of: App → Jaeger + Prometheus (2 connections)
  # You have: App → OTel Collector → Jaeger + Prometheus (1 connection)
  #
  # BENEFITS:
  # - Batching: Collector buffers and batches data
  # - Transformation: Relabel metrics, sample traces
  # - Reliability: Collector retries on failures
  # - Flexibility: Swap Jaeger for Zipkin without changing app code
  # ==========================================================================
  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.91.0
    container_name: otel-collector
    command: ["--config=/etc/otel-collector-config.yaml"]

    volumes:
      - ./config/otel-collector-config.yaml:/etc/otel-collector-config.yaml

    ports:
      - "4317:4317"  # OTLP gRPC receiver (for our app)
      - "4318:4318"  # OTLP HTTP receiver
      - "8888:8888"  # Prometheus metrics (collector's own metrics)
      - "13133:13133" # Health check endpoint

    depends_on:
      - jaeger
      - prometheus

    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:13133/"]
      interval: 10s
      timeout: 5s
      retries: 3

    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'

    networks:
      - ml-observability

    restart: unless-stopped

  # ==========================================================================
  # 4. JAEGER (Distributed Tracing)
  # ==========================================================================
  # UI: http://localhost:16686
  # ==========================================================================
  jaeger:
    image: jaegertracing/all-in-one:1.52
    container_name: jaeger

    environment:
      - COLLECTOR_OTLP_ENABLED=true
      - SPAN_STORAGE_TYPE=badger  # File-based storage (use Cassandra/Elasticsearch in prod)
      - BADGER_EPHEMERAL=false
      - BADGER_DIRECTORY_VALUE=/badger/data
      - BADGER_DIRECTORY_KEY=/badger/key

    ports:
      - "16686:16686"  # Jaeger UI
      - "14268:14268"  # Jaeger collector HTTP
      - "14250:14250"  # Jaeger collector gRPC

    volumes:
      - jaeger-data:/badger

    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'

    networks:
      - ml-observability

    restart: unless-stopped

  # ==========================================================================
  # 5. PROMETHEUS (Metrics Storage & Querying)
  # ==========================================================================
  # UI: http://localhost:9090
  # ==========================================================================
  prometheus:
    image: prom/prometheus:v2.48.0
    container_name: prometheus

    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'  # Keep 30 days of metrics
      - '--web.enable-lifecycle'  # Allow reload via HTTP POST
      - '--web.enable-admin-api'  # Enable admin API

    ports:
      - "9090:9090"

    volumes:
      - ./config/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./config/prometheus/alerts.yml:/etc/prometheus/alerts.yml
      - prometheus-data:/prometheus

    depends_on:
      - alertmanager

    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9090/-/healthy"]
      interval: 10s
      timeout: 5s
      retries: 3

    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'

    networks:
      - ml-observability

    restart: unless-stopped

  # ==========================================================================
  # 6. ALERTMANAGER (Alert Routing & Deduplication)
  # ==========================================================================
  # UI: http://localhost:9093
  # ==========================================================================
  alertmanager:
    image: prom/alertmanager:v0.26.0
    container_name: alertmanager

    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'

    ports:
      - "9093:9093"

    volumes:
      - ./config/prometheus/alertmanager.yml:/etc/alertmanager/alertmanager.yml
      - alertmanager-data:/alertmanager

    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.25'

    networks:
      - ml-observability

    restart: unless-stopped

  # ==========================================================================
  # 7. LOKI (Log Aggregation)
  # ==========================================================================
  # No UI (use Grafana to query logs)
  # ==========================================================================
  loki:
    image: grafana/loki:2.9.3
    container_name: loki

    ports:
      - "3100:3100"

    command: -config.file=/etc/loki/local-config.yaml

    volumes:
      - ./config/loki/loki-config.yaml:/etc/loki/local-config.yaml
      - loki-data:/loki

    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'

    networks:
      - ml-observability

    restart: unless-stopped

  # ==========================================================================
  # 8. PROMTAIL (Log Shipper for Loki)
  # ==========================================================================
  # Collects logs from Docker containers and ships to Loki
  # ==========================================================================
  promtail:
    image: grafana/promtail:2.9.3
    container_name: promtail

    volumes:
      - ./config/loki/promtail-config.yaml:/etc/promtail/config.yaml
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock

    command: -config.file=/etc/promtail/config.yaml

    depends_on:
      - loki

    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.25'

    networks:
      - ml-observability

    restart: unless-stopped

  # ==========================================================================
  # 9. GRAFANA (Dashboards & Visualization)
  # ==========================================================================
  # UI: http://localhost:3000 (admin/admin)
  # ==========================================================================
  grafana:
    image: grafana/grafana:10.2.3
    container_name: grafana

    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-piechart-panel

    ports:
      - "3000:3000"

    volumes:
      # Pre-provisioned datasources (Prometheus, Loki, Jaeger)
      - ./config/grafana/provisioning/datasources:/etc/grafana/provisioning/datasources

      # Pre-built dashboards
      - ./config/grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards
      - ./config/grafana/dashboards:/var/lib/grafana/dashboards

      # Persistent storage for user-created dashboards
      - grafana-data:/var/lib/grafana

    depends_on:
      - prometheus
      - loki
      - jaeger

    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3000/api/health"]
      interval: 10s
      timeout: 5s
      retries: 3

    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'

    networks:
      - ml-observability

    restart: unless-stopped

  # ==========================================================================
  # 10. KAFKA + ZOOKEEPER (Event Streaming - Optional)
  # ==========================================================================
  # For ML model retraining triggers, audit logs, etc.
  # Comment out if not needed (saves resources)
  # ==========================================================================
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

    volumes:
      - zookeeper-data:/var/lib/zookeeper/data
      - zookeeper-logs:/var/lib/zookeeper/log

    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'

    networks:
      - ml-observability

    restart: unless-stopped

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    depends_on:
      - zookeeper

    ports:
      - "9092:9092"

    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"

    volumes:
      - kafka-data:/var/lib/kafka/data

    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'

    networks:
      - ml-observability

    restart: unless-stopped

# ============================================================================
# NETWORKS
# ============================================================================
networks:
  ml-observability:
    driver: bridge
    name: ml-observability

# ============================================================================
# VOLUMES (Persistent Data)
# ============================================================================
volumes:
  redis-data:
  jaeger-data:
  prometheus-data:
  alertmanager-data:
  loki-data:
  grafana-data:
  zookeeper-data:
  zookeeper-logs:
  kafka-data:
